#!/usr/bin/env python3
"""è°ƒè¯•å‘é‡æ•°æ®åº“å†…å®¹"""

import os
import sys
sys.path.append('backend')

from langchain_community.vectorstores import Chroma
from backend.chat_agent.services.embedding_factory import EmbeddingFactory

def debug_vector_store():
    """è°ƒè¯•å‘é‡æ•°æ®åº“å†…å®¹"""
    try:
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        embeddings = EmbeddingFactory.create_embeddings()
        
        # å‘é‡æ•°æ®åº“è·¯å¾„ - ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
        vector_db_path = "backend/data/chroma/kb_1"
        
        if not os.path.exists(vector_db_path):
            print(f"âŒ å‘é‡æ•°æ®åº“ä¸å­˜åœ¨: {vector_db_path}")
            return
        
        print(f"âœ… å‘é‡æ•°æ®åº“å­˜åœ¨: {vector_db_path}")
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        vectorstore = Chroma(
            persist_directory=vector_db_path,
            embedding_function=embeddings
        )
        
        # è·å–æ‰€æœ‰æ–‡æ¡£çš„å…ƒæ•°æ®
        collection = vectorstore._collection
        all_docs = collection.get(include=["metadatas", "documents"])
        
        print(f"\nğŸ“Š å‘é‡æ•°æ®åº“ç»Ÿè®¡:")
        print(f"æ€»æ–‡æ¡£æ•°é‡: {len(all_docs['documents'])}")
        print(f"æ€»å…ƒæ•°æ®æ•°é‡: {len(all_docs['metadatas'])}")
        
        # åˆ†æå…ƒæ•°æ®
        document_ids = set()
        for i, metadata in enumerate(all_docs["metadatas"]):
            doc_id = metadata.get("document_id")
            document_ids.add(doc_id)
            
            if i < 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"\nğŸ“„ æ–‡æ¡£ {i+1}:")
                print(f"  document_id: {doc_id} (ç±»å‹: {type(doc_id)})")
                print(f"  knowledge_base_id: {metadata.get('knowledge_base_id')}")
                print(f"  chunk_id: {metadata.get('chunk_id')}")
                print(f"  chunk_index: {metadata.get('chunk_index')}")
                print(f"  å†…å®¹é¢„è§ˆ: {all_docs['documents'][i][:100]}...")
        
        # è¿‡æ»¤æ‰Noneå€¼å†æ’åº
        valid_document_ids = [doc_id for doc_id in document_ids if doc_id is not None]
        print(f"\nğŸ” å‘ç°çš„æ–‡æ¡£ID: {sorted(valid_document_ids)}")
        print(f"ğŸ” åŒ…å«Noneçš„æ–‡æ¡£ID: {document_ids}")
        
        # ä¸“é—¨æŸ¥æ‰¾æ–‡æ¡£26
        doc_26_chunks = []
        for i, metadata in enumerate(all_docs["metadatas"]):
            doc_id = metadata.get("document_id")
            if doc_id == "26" or doc_id == 26:
                doc_26_chunks.append({
                    "index": i,
                    "metadata": metadata,
                    "content": all_docs['documents'][i]
                })
        
        print(f"\nğŸ¯ æ–‡æ¡£26çš„åˆ†æ®µæ•°é‡: {len(doc_26_chunks)}")
        if len(doc_26_chunks) > 0:
            for chunk in doc_26_chunks[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"  åˆ†æ®µ {chunk['index']}: {chunk['content'][:100]}...")
                print(f"    å…ƒæ•°æ®: {chunk['metadata']}")
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£26çš„åˆ†æ®µï¼")
            # æŸ¥çœ‹æ‰€æœ‰æ–‡æ¡£IDï¼Œçœ‹çœ‹æ˜¯å¦æœ‰ç±»ä¼¼çš„
            print("\nğŸ” æ‰€æœ‰æ–‡æ¡£IDè¯¦æƒ…:")
            for i, metadata in enumerate(all_docs["metadatas"][:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
                doc_id = metadata.get("document_id")
                print(f"  ç´¢å¼•{i}: document_id={doc_id} (ç±»å‹: {type(doc_id)})")
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    debug_vector_store()