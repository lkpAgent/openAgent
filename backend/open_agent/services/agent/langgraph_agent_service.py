"""LangGraph Agent service with tool calling capabilities."""

import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.tools import tool
from langchain.chat_models import init_chat_model
# from langgraph.prebuilt import create_react_agent
from pydantic import BaseModel, Field

from .base import ToolRegistry
from open_agent.services.tools import WeatherQueryTool, TavilySearchTool, DateTimeTool
from ..postgresql_tool_manager import get_postgresql_tool
from ...core.config import get_settings
from ...utils.logger import get_logger
from ..agent_config import AgentConfigService
from open_agent.services.mcp.mcp_dynamic_tools import load_mcp_tools

logger = get_logger("langgraph_agent_service")



class LangGraphAgentConfig(BaseModel):
    """LangGraph Agent configuration."""
    model_name: str = Field(default="gpt-3.5-turbo")
    model_provider: str = Field(default="openai")
    base_url: Optional[str] = Field(default=None)
    api_key: Optional[str] = Field(default=None)
    enabled_tools: List[str] = Field(default_factory=lambda: [
        "calculator", "weather", "search", "file", "image"
    ])
    max_iterations: int = Field(default=10)
    temperature: float = Field(default=0.7)
    max_tokens: int = Field(default=1000)
    system_message: str = Field(
        default="""ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å„ç§å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€‚
                é‡è¦è§„åˆ™ï¼š
                1. å·¥å…·è°ƒç”¨å¤±è´¥æ—¶ï¼Œå¿…é¡»ä»”ç»†åˆ†æžå¤±è´¥åŽŸå› ï¼Œç‰¹åˆ«æ˜¯å‚æ•°æ ¼å¼é—®é¢˜ 
                3. åœ¨é‡æ–°è°ƒç”¨å·¥å…·å‰ï¼Œå…ˆè§£é‡Šä¸Šæ¬¡å¤±è´¥çš„åŽŸå› å’Œæ”¹è¿›æ–¹æ¡ˆ
                4. ç¡®ä¿æ¯ä¸ªå·¥å…·è°ƒç”¨çš„å‚æ•°æ ¼å¼ä¸¥æ ¼ç¬¦åˆå·¥å…·çš„è¦æ±‚ """
    )
    verbose: bool = Field(default=True)


class LangGraphAgentService:
    """LangGraph Agent service using low-level LangGraph graph (React pattern)."""
    
    def __init__(self, db_session=None):
        self.settings = get_settings()
        self.tool_registry = ToolRegistry()
        self.config = LangGraphAgentConfig()
        self.tools = []
        self.db_session = db_session
        self.config_service = AgentConfigService(db_session) if db_session else None
        self._initialize_tools()
        self._load_config()
        self._create_react_agent()
        
    def _initialize_tools(self):
        """Initialize available tools."""
        try:
            dynamic_tools = load_mcp_tools()
        except Exception as e:
            logger.warning(f"åŠ è½½ MCP åŠ¨æ€å·¥å…·å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°å·¥å…·å›žé€€: {e}")
            dynamic_tools = []

        # Always keep DateTimeTool locally
        base_tools = [DateTimeTool()]

        if dynamic_tools:
            self.tools = dynamic_tools + base_tools
            logger.info(f"LangGraph ç»‘å®š MCP åŠ¨æ€å·¥å…·: {[t.name for t in dynamic_tools]}")
        else:
            # Fallback to local weather/search when MCP not available
            self.tools = [
                WeatherQueryTool(),
                TavilySearchTool(),
            ] + base_tools
            logger.info("MCP ä¸å¯ç”¨ï¼Œå·²å›žé€€åˆ°æœ¬åœ° Weather/Search å·¥å…·")


            
    def _load_config(self):
        """Load configuration from database if available."""
        if self.config_service:
            try:
                db_config = self.config_service.get_active_config()
                if db_config:
                    # Update config with database values
                    config_dict = db_config.config_data
                    for key, value in config_dict.items():
                        if hasattr(self.config, key):
                            setattr(self.config, key, value)
                    logger.info("Loaded configuration from database")
            except Exception as e:
                logger.warning(f"Failed to load config from database: {e}")
                

        
    def _create_react_agent(self):
        """Create LangGraph agent using low-level StateGraph with explicit nodes/edges."""
        try:
            # Initialize the model
            llm_config = get_settings().llm.get_current_config()
            self.model = init_chat_model(
                model=llm_config['model'],
                model_provider='openai',
                temperature=llm_config['temperature'],
                max_tokens=llm_config['max_tokens'],
                base_url= llm_config['base_url'],
                api_key=llm_config['api_key']
            )
            
            # Bind tools to the model so it can propose tool calls
            try:
                self.bound_model = self.model.bind_tools(self.tools)
            except Exception as e:
                logger.warning(f"Failed to bind tools to model, tool calling may not work: {e}")
                self.bound_model = self.model

            # Build low-level React graph: State -> agent -> tools -> agent ... until stop
            from typing import TypedDict
            from langgraph.graph import StateGraph, START, END
            from langchain_core.messages import ToolMessage, BaseMessage
            from typing import Annotated
            from langgraph.graph.message import add_messages

            class AgentState(TypedDict):
                messages: Annotated[List[BaseMessage], add_messages]

            # Node: call the model
            def agent_node(state: AgentState) -> AgentState:
                messages = state["messages"]
                # Optionally include a system instruction at the start for first turn
                if messages and messages[0].__class__.__name__ != 'SystemMessage':
                    # Keep user history untouched; rely on upstream to include system if desired
                    pass
                ai = self.bound_model.invoke(messages)
                return {"messages": [ai]}

            # Node: execute tools requested by the last AI message
            def tools_node(state: AgentState) -> AgentState:
                messages = state["messages"]
                last = messages[-1]
                outputs: List[ToolMessage] = []
                try:
                    tool_calls = getattr(last, 'tool_calls', []) or []
                    tool_map = {t.name: t for t in self.tools}
                    for call in tool_calls:
                        name = call.get('name') if isinstance(call, dict) else getattr(call, 'name', None)
                        args = call.get('args') if isinstance(call, dict) else getattr(call, 'args', {})
                        call_id = call.get('id') if isinstance(call, dict) else getattr(call, 'id', '')
                        if name in tool_map:
                            try:
                                result = tool_map[name].invoke(args)
                            except Exception as te:
                                result = f"Tool {name} execution error: {te}"
                        else:
                            result = f"Unknown tool: {name}"
                        outputs.append(ToolMessage(content=str(result), tool_call_id=call_id))
                except Exception as e:
                    outputs.append(ToolMessage(content=f"Tool execution error: {e}", tool_call_id=""))
                return {"messages": outputs}

            # Router: decide next step after agent node
            def route_after_agent(state: AgentState) -> str:
                last = state["messages"][-1]
                finish_reason = None
                try:
                    meta = getattr(last, 'response_metadata', {}) or {}
                    finish_reason = meta.get('finish_reason')
                except Exception:
                    finish_reason = None
                # If the model decided to call tools, continue to tools node
                if getattr(last, 'tool_calls', None):
                    return "tools"
                # Otherwise, end
                return END

            graph = StateGraph(AgentState)
            graph.add_node("agent", agent_node)
            graph.add_node("tools", tools_node)
            graph.add_edge(START, "agent")
            graph.add_conditional_edges("agent", route_after_agent, {"tools": "tools", END: END})
            graph.add_edge("tools", "agent")
            
            # Compile graph and store as self.agent for compatibility with existing code
            self.react_agent = graph.compile()
            
            logger.info("LangGraph low-level React agent created successfully")
            
        except Exception as e:
            logger.error(f"Failed to create agent: {str(e)}")
            raise
        

            

            

        
    def _format_tools_info(self) -> str:
        """Format tools information for the prompt."""
        tools_info = []
        for tool_name in self.config.enabled_tools:
            tool = self.tool_registry.get_tool(tool_name)
            if tool:
                params_info = []
                for param in tool.get_parameters():
                    params_info.append(f"  - {param.name} ({param.type.value}): {param.description}")
                
                tool_info = f"**{tool.get_name()}**: {tool.get_description()}"
                if params_info:
                    tool_info += "\n" + "\n".join(params_info)
                tools_info.append(tool_info)
                
        return "\n\n".join(tools_info)
        

        
    async def chat(self, message: str, chat_history: Optional[List[Dict[str, str]]] = None) -> Dict[str, Any]:
        """Process a chat message using LangGraph."""
        try:
            logger.info(f"Starting chat with message: {message[:100]}...")
            
            # Convert chat history to messages
            messages = []
            if chat_history:
                for msg in chat_history:
                    if msg["role"] == "user":
                        messages.append(HumanMessage(content=msg["content"]))
                    elif msg["role"] == "assistant":
                        messages.append(AIMessage(content=msg["content"]))
                        
            # Add current message
            messages.append(HumanMessage(content=message))
            
            # Use the low-level graph directly
            result = await self.react_agent.ainvoke({"messages": messages}, {"recursion_limit": 6}, )
            
            # Extract final response
            final_response = ""
            if "messages" in result and result["messages"]:
                last_message = result["messages"][-1]
                if hasattr(last_message, 'content'):
                    final_response = last_message.content
                elif isinstance(last_message, dict) and "content" in last_message:
                    final_response = last_message["content"]
                    
            return {
                "response": final_response,
                "intermediate_steps": [],
                "success": True,
                "error": None
            }
            
        except Exception as e:
            logger.error(f"LangGraph chat error: {str(e)}", exc_info=True)
            return {
                "response": f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºçŽ°é”™è¯¯: {str(e)}",
                "intermediate_steps": [],
                "success": False,
                "error": str(e)
            }

    async def chat_stream(self, message: str, chat_history: Optional[List[Dict[str, str]]] = None) -> AsyncGenerator[
        Dict[str, Any], None]:
        """Process a chat message using LangGraph with streaming."""
        try:
            logger.info(f"Starting streaming chat with message: {message[:100]}...")

            # Convert chat history to messages
            messages = []
            if chat_history:
                for msg in chat_history:
                    if msg["role"] == "user":
                        messages.append(HumanMessage(content=msg["content"]))
                    elif msg["role"] == "assistant":
                        messages.append(AIMessage(content=msg["content"]))

            # Add current message
            messages.append(HumanMessage(content=message))

            # Track state for streaming
            intermediate_steps = []
            final_response_started = False
            accumulated_response = ""
            final_ai_message = None

            # Stream the agent execution
            async for event in self.react_agent.astream({"messages": messages}):
                # Handle different event types from LangGraph
                print('event===', event)
                if isinstance(event, dict):
                    for node_name, node_output in event.items():
                        logger.info(f"Processing node: {node_name}, output type: {type(node_output)}")

                        # å¤„ç† tools èŠ‚ç‚¹
                        if "tools" in node_name.lower():
                            # æå–å·¥å…·ä¿¡æ¯
                            tool_infos = []

                            if isinstance(node_output, dict) and "messages" in node_output:
                                messages_in_output = node_output["messages"]

                                for msg in messages_in_output:
                                    # å¤„ç† ToolMessage å¯¹è±¡
                                    if hasattr(msg, 'name') and hasattr(msg, 'content'):
                                        tool_info = {
                                            "tool_name": msg.name,
                                            "tool_output": msg.content,
                                            "tool_call_id": getattr(msg, 'tool_call_id', ''),
                                            "status": "completed"
                                        }
                                        tool_infos.append(tool_info)
                                    elif isinstance(msg, dict):
                                        if 'name' in msg and 'content' in msg:
                                            tool_info = {
                                                "tool_name": msg['name'],
                                                "tool_output": msg['content'],
                                                "tool_call_id": msg.get('tool_call_id', ''),
                                                "status": "completed"
                                            }
                                            tool_infos.append(tool_info)

                            # è¿”å›ž tools_end äº‹ä»¶
                            for tool_info in tool_infos:
                                yield {
                                    "type": "tools_end",
                                    "content": f"å·¥å…· {tool_info['tool_name']} æ‰§è¡Œå®Œæˆ",
                                    "tool_name": tool_info["tool_name"],
                                    "tool_output": tool_info["tool_output"],
                                    "node_name": node_name,
                                    "done": False
                                }
                                await asyncio.sleep(0.1)

                        # å¤„ç† agent èŠ‚ç‚¹
                        elif "agent" in node_name.lower():
                            if isinstance(node_output, dict) and "messages" in node_output:
                                messages_in_output = node_output["messages"]
                                if messages_in_output:
                                    last_msg = messages_in_output[-1]

                                    # èŽ·å– finish_reason
                                    finish_reason = None
                                    if hasattr(last_msg, 'response_metadata'):
                                        finish_reason = last_msg.response_metadata.get('finish_reason')
                                    elif isinstance(last_msg, dict) and 'response_metadata' in last_msg:
                                        finish_reason = last_msg['response_metadata'].get('finish_reason')

                                    # åˆ¤æ–­æ˜¯å¦ä¸º thinking æˆ– response
                                    if finish_reason == 'tool_calls':
                                        # thinking çŠ¶æ€
                                        thinking_content = "ðŸ¤” æ­£åœ¨æ€è€ƒ..."
                                        if hasattr(last_msg, 'content') and last_msg.content:
                                            thinking_content = f"ðŸ¤” æ€è€ƒ: {last_msg.content[:200]}..."
                                        elif isinstance(last_msg, dict) and "content" in last_msg:
                                            thinking_content = f"ðŸ¤” æ€è€ƒ: {last_msg['content'][:200]}..."

                                        yield {
                                            "type": "thinking",
                                            "content": thinking_content,
                                            "node_name": node_name,
                                            "raw_output": str(node_output)[:500] if node_output else "",
                                            "done": False
                                        }
                                        await asyncio.sleep(0.1)

                                    elif finish_reason == 'stop':
                                        # response çŠ¶æ€
                                        if hasattr(last_msg, 'content') and hasattr(last_msg,
                                                                                    '__class__') and 'AI' in last_msg.__class__.__name__:
                                            current_content = last_msg.content
                                            final_ai_message = last_msg

                                            if not final_response_started and current_content:
                                                final_response_started = True
                                                yield {
                                                    "type": "response_start",
                                                    "content": "",
                                                    "intermediate_steps": intermediate_steps,
                                                    "done": False
                                                }

                                            if current_content and len(current_content) > len(accumulated_response):
                                                new_content = current_content[len(accumulated_response):]

                                                for char in new_content:
                                                    accumulated_response += char
                                                    yield {
                                                        "type": "response",
                                                        "content": accumulated_response,
                                                        "intermediate_steps": intermediate_steps,
                                                        "done": False
                                                    }
                                                    await asyncio.sleep(0.03)

                                    else:
                                        # å…¶ä»– agent çŠ¶æ€
                                        yield {
                                            "type": "step",
                                            "content": f"ðŸ“‹ æ‰§è¡Œæ­¥éª¤: {node_name}",
                                            "node_name": node_name,
                                            "raw_output": str(node_output)[:500] if node_output else "",
                                            "done": False
                                        }
                                        await asyncio.sleep(0.1)

                        # å¤„ç†å…¶ä»–èŠ‚ç‚¹
                        else:
                            yield {
                                "type": "step",
                                "content": f"ðŸ“‹ æ‰§è¡Œæ­¥éª¤: {node_name}",
                                "node_name": node_name,
                                "raw_output": str(node_output)[:500] if node_output else "",
                                "done": False
                            }
                            await asyncio.sleep(0.1)

            # æœ€ç»ˆå®Œæˆäº‹ä»¶
            yield {
                "type": "complete",
                "content": accumulated_response,
                "intermediate_steps": intermediate_steps,
                "done": True
            }

        except Exception as e:
            logger.error(f"Error in chat_stream: {str(e)}", exc_info=True)
            yield {
                "type": "error",
                "content": f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}",
                "done": True
            }
                                        
            # ç¡®ä¿æœ€ç»ˆå“åº”åŒ…å«å®Œæ•´å†…å®¹
            final_content = accumulated_response
            if not final_content and final_ai_message and hasattr(final_ai_message, 'content'):
                final_content = final_ai_message.content or ""
            
            # Final completion signal
            yield {
                "type": "response",
                "content": final_content,
                "intermediate_steps": intermediate_steps,
                "done": True
            }
            
        except Exception as e:
            logger.error(f"LangGraph chat stream error: {str(e)}", exc_info=True)
            yield {
                "type": "error",
                "content": f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºçŽ°é”™è¯¯: {str(e)}",
                "error": str(e),
                "done": True
            }
            
    def get_available_tools(self) -> List[Dict[str, Any]]:
        """Get list of available tools."""
        tools = []
        for tool in self.tools:
            tools.append({
                "name": tool.name,
                "description": tool.description,
                "parameters": [],
                "enabled": True
            })
        return tools
        
    def get_config(self) -> Dict[str, Any]:
        """Get current agent configuration."""
        return self.config.dict()
        
    def update_config(self, config: Dict[str, Any]):
        """Update agent configuration."""
        for key, value in config.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
        
        # Recreate agent with new config
        self._create_react_agent()
        logger.info("Agent configuration updated")

    def _create_plan_execute_agent(self):
        """Create a Plan-and-Execute agent using LangGraph low-level API.
        ç»“æž„ï¼šSTART -> planner -> executor(loop) -> summarize -> END
        - plannerï¼šæ ¹æ®ç”¨æˆ·é—®é¢˜ç”Ÿæˆè®¡åˆ’ï¼ˆJSON æ•°ç»„ï¼‰
        - executorï¼šé€æ­¥æ‰§è¡Œè®¡åˆ’ï¼ˆå¯è°ƒç”¨å·¥å…·ï¼‰ï¼Œæ”¶é›†æ¯æ­¥ç»“æžœ
        - summarizeï¼šç»¼åˆè®¡åˆ’ä¸Žæ‰§è¡Œç»“æžœäº§å‡ºæœ€ç»ˆå›žç­”
        """
        from typing import TypedDict, Annotated, List
        import json
        from langgraph.graph import StateGraph, START, END
        from langgraph.graph.message import add_messages
        from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, BaseMessage
        try:
            self.bound_model = self.model.bind_tools(self.tools)
        except Exception as e:
            logger.warning(f"Failed to bind tools to model, tool calling may not work: {e}")
            self.bound_model = self.model
        class PlanState(TypedDict):
            messages: Annotated[List[BaseMessage], add_messages]
            plan_steps: List[str]
            current_step: int
            step_results: List[str]

        def planner_node(state: PlanState) -> PlanState:
            messages = state.get("messages", [])
            plan_prompt = (
                "ä½ æ˜¯è§„åˆ’åŠ©æ‰‹ã€‚åŸºäºŽå¯¹è¯å†…å®¹ç”Ÿæˆå¯æ‰§è¡Œè®¡åˆ’ï¼Œ" 
                "ç”¨ JSON æ•°ç»„è¿”å›žï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€æ¡æ˜Žç¡®ä¸”å¯æ“ä½œçš„æ­¥éª¤ã€‚" 
                "ä»…è¾“å‡º JSONï¼Œä¸è¦é¢å¤–è§£é‡Šã€‚"
            )
            ai_plan = self.model.invoke(messages + [HumanMessage(content=plan_prompt)])
            steps: List[str] = []
            try:
                parsed = json.loads(ai_plan.content)
                if isinstance(parsed, list):
                    steps = [str(s) for s in parsed]
            except Exception:
                # å›žé€€ï¼šæŒ‰è¡Œæ‹†åˆ†
                steps = [s.strip() for s in ai_plan.content.split("\n") if s.strip()]
            return {
                "messages": [ai_plan],
                "plan_steps": steps,
                "current_step": 0,
                "step_results": []
            }

        def executor_node(state: PlanState) -> PlanState:
            idx = state.get("current_step", 0)
            steps = state.get("plan_steps", [])
            msgs = state.get("messages", [])
            if idx >= len(steps):
                return {"messages": [], "current_step": idx, "step_results": state.get("step_results", [])}

            step_text = steps[idx]
            exec_prompt = (
                f"è¯·æ‰§è¡Œè®¡åˆ’çš„ç¬¬{idx+1}æ­¥ï¼š{step_text}ã€‚" 
                "éœ€è¦ç”¨å·¥å…·æ—¶åˆ›å»ºå·¥å…·è°ƒç”¨ï¼›å®ŒæˆåŽç»™å‡ºè¯¥æ­¥çš„ç»“æžœã€‚"
            )
            ai_exec = self.bound_model.invoke(msgs + [HumanMessage(content=exec_prompt)])

            new_messages: List[BaseMessage] = [ai_exec]
            step_result_content = None

            # å¤„ç†å·¥å…·è°ƒç”¨
            tool_map = {t.name: t for t in self.tools}
            tool_msgs: List[ToolMessage] = []
            tool_calls = getattr(ai_exec, "tool_calls", []) or (ai_exec.additional_kwargs.get("tool_calls") if hasattr(ai_exec, "additional_kwargs") else [])
            if tool_calls:
                for call in tool_calls:
                    name = call.get("name")
                    args = call.get("args", {})
                    tool_obj = tool_map.get(name)
                    if tool_obj:
                        try:
                            result = tool_obj.invoke(args)
                        except Exception as e:
                            result = f"å·¥å…·æ‰§è¡Œå¤±è´¥: {e}"
                    else:
                        result = f"æœªæ‰¾åˆ°å·¥å…·: {name}"
                    tool_call_id = call.get("id") or call.get("tool_call_id") or call.get("call_id") or f"tool_{name}"
                    tool_msgs.append(ToolMessage(content=str(result), tool_call_id=tool_call_id, name=name or "tool"))
                new_messages.extend(tool_msgs)
                # åŸºäºŽå·¥å…·è¾“å‡ºæ€»ç»“è¯¥æ­¥ç»“æžœ
                summarize_step = "è¯·åŸºäºŽä¸Šè¿°å·¥å…·è¾“å‡ºï¼Œæ€»ç»“è¯¥æ­¥éª¤çš„ç»“æžœï¼Œç»™å‡ºç»“æž„åŒ–è¦ç‚¹ä¸Žå¯è¯»è¯´æ˜Žã€‚"
                ai_step = self.bound_model.invoke(msgs + [ai_exec] + tool_msgs + [HumanMessage(content=summarize_step)])
                step_result_content = ai_step.content
                new_messages.append(ai_step)
            else:
                step_result_content = ai_exec.content

            all_results = list(state.get("step_results", []))
            if step_result_content:
                all_results.append(step_result_content)

            return {
                "messages": new_messages,
                "current_step": idx + 1,
                "step_results": all_results
            }

        def route_after_planner(state: PlanState) -> str:
            return "executor" if state.get("plan_steps") else END

        def route_after_executor(state: PlanState) -> str:
            cur = state.get("current_step", 0)
            total = len(state.get("plan_steps", []))
            return "executor" if cur < total else "summarize"

        def summarize_node(state: PlanState) -> PlanState:
            import json as _json
            msgs = state.get("messages", [])
            steps = state.get("plan_steps", [])
            results = state.get("step_results", [])
            final_prompt = (
                "è¯·ç»¼åˆä»¥ä¸Šè®¡åˆ’ä¸Žå„æ­¥éª¤ç»“æžœï¼Œç”Ÿæˆæœ€ç»ˆå›žç­”ã€‚" 
                "è¦æ±‚ï¼šé€»è¾‘æ¸…æ™°ã€ç»“è®ºæ˜Žç¡®ã€å¯è¯»æ€§å¼ºï¼›å¦‚å­˜åœ¨ä¸ç¡®å®šæ€§è¯·æ³¨æ˜Žã€‚"
            )
            context_msg = HumanMessage(content=(
                f"è®¡åˆ’: {_json.dumps(steps, ensure_ascii=False)}\n"
                f"æ­¥éª¤ç»“æžœ: {_json.dumps(results, ensure_ascii=False)}\n"
                f"{final_prompt}"
            ))
            ai_final = self.model.invoke(msgs + [context_msg])
            return {"messages": [ai_final]}

        graph = StateGraph(PlanState)
        graph.add_node("planner", planner_node)
        graph.add_node("executor", executor_node)
        graph.add_node("summarize", summarize_node)
        graph.add_edge(START, "planner")
        graph.add_conditional_edges("planner", route_after_planner, {"executor": "executor", END: END})
        graph.add_conditional_edges("executor", route_after_executor, {"executor": "executor", "summarize": "summarize"})
        graph.add_edge("summarize", END)

        self.plan_execute_agent = graph.compile()

    async def chat_plan_execute(self, message: str, chat_history: Optional[List[Dict[str, str]]] = None) -> Dict[str, Any]:
        """Single-turn Plan-and-Execute chat."""
        # ç¡®ä¿ agent å·²åˆ›å»º
        if not hasattr(self, "plan_execute_agent"):
            self._create_plan_execute_agent()

        # æž„å»ºæ¶ˆæ¯
        messages = []
        if chat_history:
            for msg in chat_history:
                role = msg.get("role")
                content = msg.get("content", "")
                if role == "user":
                    messages.append(HumanMessage(content=content))
                else:
                    messages.append(AIMessage(content=content))
        messages.append(HumanMessage(content=message))

        try:
            result = await self.plan_execute_agent.ainvoke({"messages": messages}, config={"recursion_limit": self.config.max_iterations})
            final_msg = None
            if isinstance(result, dict) and "messages" in result:
                ms = result["messages"]
                if ms:
                    final_msg = ms[-1]
            final_text = getattr(final_msg, "content", "") if final_msg else ""
            return {
                "status": "success",
                "response": final_text,
                "raw": str(result)
            }
        except Exception as e:
            logger.error(f"Error in chat_plan_execute: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e)
            }

    async def chat_stream_plan_execute(self, message: str, chat_history: Optional[List[Dict[str, str]]] = None) -> AsyncGenerator[Dict[str, Any], None]:
        """Streamed Plan-and-Execute chat."""
        import asyncio as _asyncio
        if not hasattr(self, "plan_execute_agent"):
            self._create_plan_execute_agent()

        messages = []
        if chat_history:
            for msg in chat_history:
                role = msg.get("role")
                content = msg.get("content", "")
                if role == "user":
                    messages.append(HumanMessage(content=content))
                else:
                    messages.append(AIMessage(content=content))
        messages.append(HumanMessage(content=message))

        try:
            accumulated = ""
            async for event in self.react_agent.astream({"messages": messages}, config={"recursion_limit": self.config.max_iterations}):
                for key, node_output in event.items():
                    node_name = key[0] if isinstance(key, tuple) else key
                    if node_name == "planner":
                        # è§„åˆ’é˜¶æ®µ
                        content = "ç”Ÿæˆè®¡åˆ’ä¸­..."
                        if node_output and isinstance(node_output, dict):
                            m = node_output.get("messages", [])
                            if m:
                                last = m[-1]
                                if hasattr(last, "content"):
                                    content = str(last.content)[:400]
                        yield {"type": "planning", "content": content, "done": False}
                        await _asyncio.sleep(0.05)
                    elif node_name == "executor":
                        # æ‰§è¡Œé˜¶æ®µï¼ˆå¯èƒ½åŒ…å«å·¥å…·ï¼‰
                        yield {"type": "step", "content": "æ‰§è¡Œè®¡åˆ’æ­¥éª¤", "done": False}
                        await _asyncio.sleep(0.05)
                        if node_output and isinstance(node_output, dict):
                            msgs = node_output.get("messages", [])
                            # è¾“å‡ºå·¥å…·ç»“æŸæ ‡è®°
                            tool_msgs = [m for m in msgs if hasattr(m, "__class__") and "Tool" in m.__class__.__name__]
                            if tool_msgs:
                                yield {"type": "tools_end", "content": f"å®Œæˆ {len(tool_msgs)} æ¬¡å·¥å…·æ‰§è¡Œ", "done": False}
                                await _asyncio.sleep(0.03)
                            # å°è¯•è¾“å‡ºè¯¥æ­¥æ€»ç»“
                            ai_msgs = [m for m in msgs if hasattr(m, "__class__") and "AI" in m.__class__.__name__]
                            if ai_msgs:
                                text = ai_msgs[-1].content
                                if text:
                                    accumulated = text
                                    yield {"type": "response", "content": accumulated, "done": False}
                                    await _asyncio.sleep(0.02)
                    elif node_name == "summarize":
                        # æœ€ç»ˆæ€»ç»“
                        if node_output and isinstance(node_output, dict):
                            msgs = node_output.get("messages", [])
                            if msgs:
                                final = msgs[-1]
                                content = getattr(final, "content", "")
                                if content:
                                    yield {"type": "response_start", "content": "", "done": False}
                                    yield {"type": "response", "content": content, "done": False}
                                    accumulated = content
                                    await _asyncio.sleep(0.02)
            yield {"type": "complete", "content": accumulated, "done": True}
        except Exception as e:
            logger.error(f"Error in chat_stream_plan_execute: {e}", exc_info=True)
            yield {"type": "error", "content": str(e), "done": True}


# Global instance
_langgraph_agent_service: Optional[LangGraphAgentService] = None


def get_langgraph_agent_service(db_session=None) -> LangGraphAgentService:
    """Get or create LangGraph agent service instance."""
    global _langgraph_agent_service
    
    if _langgraph_agent_service is None:
        _langgraph_agent_service = LangGraphAgentService(db_session)
        logger.info("LangGraph Agent service initialized")
        
    return _langgraph_agent_service