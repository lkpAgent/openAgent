#!/usr/bin/env python3
"""æµ‹è¯•ä¿®æ”¹åçš„èŠå¤©åŠŸèƒ½æ˜¯å¦èƒ½æ­£å¸¸ä½¿ç”¨PGVectorè¿›è¡ŒçŸ¥è¯†åº“é—®ç­”"""

import sys
import os
import asyncio
from sqlalchemy.orm import Session

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from open_agent.db.database import get_db
from open_agent.services.knowledge_chat import KnowledgeChatService
from open_agent.services.conversation import ConversationService
from open_agent.services.document_processor import get_document_processor
from open_agent.utils.logger import get_logger
from open_agent.core.config import settings
from langchain.schema import Document

logger = get_logger("test_chat_pgvector")

async def test_chat_with_pgvector():
    """æµ‹è¯•èŠå¤©åŠŸèƒ½ä½¿ç”¨PGVector"""
    try:
        # è·å–æ•°æ®åº“ä¼šè¯
        db = next(get_db())
        
        # åˆå§‹åŒ–æœåŠ¡
        knowledge_chat_service = KnowledgeChatService(db)
        conversation_service = ConversationService(db)
        doc_processor = get_document_processor()
        
        logger.info("å¼€å§‹æµ‹è¯•èŠå¤©åŠŸèƒ½ä½¿ç”¨PGVector...")
        
        # æµ‹è¯•çŸ¥è¯†åº“ID
        knowledge_base_id = 999
        
        # 1. é¦–å…ˆæ·»åŠ ä¸€äº›æµ‹è¯•æ–‡æ¡£åˆ°çŸ¥è¯†åº“
        logger.info("æ·»åŠ æµ‹è¯•æ–‡æ¡£åˆ°çŸ¥è¯†åº“...")
        test_documents = [
            Document(
                page_content="Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå…·æœ‰ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½ã€‚å®ƒå¹¿æ³›åº”ç”¨äºWebå¼€å‘ã€æ•°æ®ç§‘å­¦ã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚",
                metadata={
                    "source": "python_intro.txt",
                    "filename": "python_intro.txt"
                }
            ),
            Document(
                page_content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚å¸¸è§çš„æœºå™¨å­¦ä¹ ç®—æ³•åŒ…æ‹¬çº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€ç¥ç»ç½‘ç»œç­‰ã€‚",
                metadata={
                    "source": "ml_intro.txt",
                    "filename": "ml_intro.txt"
                }
            ),
            Document(
                page_content="æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚æ¨¡å¼ã€‚å®ƒåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚",
                metadata={
                    "source": "dl_intro.txt",
                    "filename": "dl_intro.txt"
                }
            )
        ]
        
        # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
        doc_processor.add_documents_to_vector_store(knowledge_base_id, test_documents, document_id=888)
        logger.info("æµ‹è¯•æ–‡æ¡£æ·»åŠ å®Œæˆ")
        
        # 2. åˆ›å»ºä¸€ä¸ªæµ‹è¯•å¯¹è¯
        logger.info("åˆ›å»ºæµ‹è¯•å¯¹è¯...")
        # è¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿä¸€ä¸ªå¯¹è¯IDï¼Œå®é™…åº”ç”¨ä¸­åº”è¯¥é€šè¿‡APIåˆ›å»º
        conversation_id = 1  # å‡è®¾å­˜åœ¨IDä¸º1çš„å¯¹è¯
        
        # 3. æµ‹è¯•å‘é‡å­˜å‚¨è·å–
        logger.info("æµ‹è¯•å‘é‡å­˜å‚¨è·å–...")
        vector_store = knowledge_chat_service._get_vector_store(knowledge_base_id)
        if vector_store:
            logger.info(f"âœ… æˆåŠŸè·å–å‘é‡å­˜å‚¨: {type(vector_store).__name__}")
            
            # æµ‹è¯•ç›¸ä¼¼æ€§æœç´¢
            logger.info("æµ‹è¯•ç›¸ä¼¼æ€§æœç´¢...")
            search_results = vector_store.similarity_search("ä»€ä¹ˆæ˜¯Python", k=2)
            logger.info(f"æœç´¢åˆ° {len(search_results)} ä¸ªç›¸å…³æ–‡æ¡£")
            for i, doc in enumerate(search_results):
                logger.info(f"  æ–‡æ¡£ {i+1}: {doc.page_content[:50]}...")
        else:
            logger.error("âŒ æ— æ³•è·å–å‘é‡å­˜å‚¨")
            return False
        
        # 4. æµ‹è¯•çŸ¥è¯†åº“æœç´¢åŠŸèƒ½
        logger.info("æµ‹è¯•çŸ¥è¯†åº“æœç´¢åŠŸèƒ½...")
        search_results = await knowledge_chat_service.search_knowledge_base(
            knowledge_base_id=knowledge_base_id,
            query="æœºå™¨å­¦ä¹ æ˜¯ä»€ä¹ˆ",
            k=3
        )
        
        if search_results:
            logger.info(f"âœ… çŸ¥è¯†åº“æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³æ–‡æ¡£")
            for i, result in enumerate(search_results):
                logger.info(f"  ç»“æœ {i+1}: ç›¸ä¼¼åº¦={result.get('similarity_score', 'N/A'):.4f}, å†…å®¹={result['content'][:50]}...")
        else:
            logger.warning("çŸ¥è¯†åº“æœç´¢æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        
        # 5. æµ‹è¯•æµå¼èŠå¤©åŠŸèƒ½ï¼ˆæ¨¡æ‹Ÿï¼‰
        logger.info("æµ‹è¯•æµå¼èŠå¤©åŠŸèƒ½...")
        test_message = "è¯·ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€"
        
        try:
            # ç”±äºæˆ‘ä»¬æ²¡æœ‰çœŸå®çš„å¯¹è¯è®°å½•ï¼Œè¿™é‡Œåªæµ‹è¯•å‘é‡å­˜å‚¨éƒ¨åˆ†
            # å®é™…çš„èŠå¤©åŠŸèƒ½éœ€è¦å®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
            logger.info("æ¨¡æ‹ŸèŠå¤©æŸ¥è¯¢...")
            
            # è·å–ç›¸å…³æ–‡æ¡£
            retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 3})
            relevant_docs = retriever.get_relevant_documents(test_message)
            
            logger.info(f"âœ… æ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç”¨äºå›ç­”")
            for i, doc in enumerate(relevant_docs):
                logger.info(f"  ç›¸å…³æ–‡æ¡£ {i+1}: {doc.page_content[:50]}...")
                
        except Exception as e:
            logger.error(f"âŒ æµå¼èŠå¤©æµ‹è¯•å¤±è´¥: {e}")
            return False
        
        # 6. æ¸…ç†æµ‹è¯•æ•°æ®
        logger.info("æ¸…ç†æµ‹è¯•æ•°æ®...")
        doc_processor.delete_document_from_vector_store(knowledge_base_id, 888)
        logger.info("æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
        
        logger.info("ğŸ‰ èŠå¤©åŠŸèƒ½PGVectoré›†æˆæµ‹è¯•å®Œæˆï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return False
    finally:
        if 'db' in locals():
            db.close()

if __name__ == "__main__":
    success = asyncio.run(test_chat_with_pgvector())
    if success:
        print("\nâœ… èŠå¤©åŠŸèƒ½PGVectoré›†æˆæµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâŒ èŠå¤©åŠŸèƒ½PGVectoré›†æˆæµ‹è¯•å¤±è´¥ï¼")
        sys.exit(1)