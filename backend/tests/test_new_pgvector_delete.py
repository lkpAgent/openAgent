#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•æ–°ç‰ˆæœ¬PGVectoråˆ é™¤åŠŸèƒ½
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from langchain_postgres import PGVector
from langchain.schema import Document
from open_agent.core.config import settings
from open_agent.services.embedding_factory import EmbeddingFactory
from urllib.parse import quote

def test_pgvector_delete():
    """æµ‹è¯•æ–°ç‰ˆæœ¬PGVectorçš„åˆ é™¤åŠŸèƒ½"""
    print("å¼€å§‹æµ‹è¯•æ–°ç‰ˆæœ¬PGVectoråˆ é™¤åŠŸèƒ½...")
    
    # åˆå§‹åŒ–embedding
    embeddings = EmbeddingFactory.create_embeddings()
    
    # æ„å»ºè¿æ¥å­—ç¬¦ä¸²
    connection_string = (
        f"postgresql+psycopg://{settings.vector_db.pgvector_user}:"
        f"{quote(settings.vector_db.pgvector_password)}@"
        f"{settings.vector_db.pgvector_host}:"
        f"{settings.vector_db.pgvector_port}/"
        f"{settings.vector_db.pgvector_database}"
    )
    
    # æµ‹è¯•é›†åˆåç§°
    collection_name = "test_delete_collection"
    
    try:
        # åˆ›å»ºPGVectorå®ä¾‹
        vector_store = PGVector(
            connection=connection_string,
            embeddings=embeddings,
            collection_name=collection_name,
            use_jsonb=True
        )
        
        print(f"æˆåŠŸè¿æ¥åˆ°PGVectorï¼Œé›†åˆåç§°: {collection_name}")
        
        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        test_documents = [
            Document(
                page_content="è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£çš„å†…å®¹",
                metadata={
                    "document_id": "test_doc_1",
                    "knowledge_base_id": 999,
                    "chunk_id": "999_test_doc_1_0",
                    "chunk_index": 0
                }
            ),
            Document(
                page_content="è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•æ–‡æ¡£çš„å†…å®¹",
                metadata={
                    "document_id": "test_doc_1",
                    "knowledge_base_id": 999,
                    "chunk_id": "999_test_doc_1_1",
                    "chunk_index": 1
                }
            ),
            Document(
                page_content="è¿™æ˜¯å¦ä¸€ä¸ªæ–‡æ¡£çš„å†…å®¹",
                metadata={
                    "document_id": "test_doc_2",
                    "knowledge_base_id": 999,
                    "chunk_id": "999_test_doc_2_0",
                    "chunk_index": 0
                }
            )
        ]
        
        # æ·»åŠ æµ‹è¯•æ–‡æ¡£
        print("æ·»åŠ æµ‹è¯•æ–‡æ¡£...")
        vector_store.add_documents(test_documents)
        print(f"æˆåŠŸæ·»åŠ  {len(test_documents)} ä¸ªæµ‹è¯•æ–‡æ¡£")
        
        # éªŒè¯æ–‡æ¡£å·²æ·»åŠ 
        print("\néªŒè¯æ–‡æ¡£å·²æ·»åŠ ...")
        search_results = vector_store.similarity_search(
            query="æµ‹è¯•æ–‡æ¡£",
            k=10,
            filter={"knowledge_base_id": {"$eq": 999}}
        )
        print(f"æœç´¢åˆ° {len(search_results)} ä¸ªæ–‡æ¡£")
        for i, doc in enumerate(search_results):
            print(f"  æ–‡æ¡£ {i+1}: document_id={doc.metadata.get('document_id')}, å†…å®¹={doc.page_content[:30]}...")
        
        # æµ‹è¯•åˆ é™¤åŠŸèƒ½
        print("\næµ‹è¯•åˆ é™¤åŠŸèƒ½...")
        print("åˆ é™¤ document_id='test_doc_1' çš„æ‰€æœ‰æ–‡æ¡£")
        
        # ä½¿ç”¨æ–°ç‰ˆæœ¬çš„è¿‡æ»¤å™¨è¯­æ³•åˆ é™¤
        filter_condition = {"document_id": {"$eq": "test_doc_1"}}
        vector_store.delete(filter=filter_condition)
        print("åˆ é™¤æ“ä½œå®Œæˆ")
        
        # éªŒè¯åˆ é™¤ç»“æœ
        print("\néªŒè¯åˆ é™¤ç»“æœ...")
        remaining_results = vector_store.similarity_search(
            query="æµ‹è¯•æ–‡æ¡£",
            k=10,
            filter={"knowledge_base_id": {"$eq": 999}}
        )
        print(f"åˆ é™¤åå‰©ä½™ {len(remaining_results)} ä¸ªæ–‡æ¡£")
        for i, doc in enumerate(remaining_results):
            print(f"  å‰©ä½™æ–‡æ¡£ {i+1}: document_id={doc.metadata.get('document_id')}, å†…å®¹={doc.page_content[:30]}...")
        
        # æ£€æŸ¥æ˜¯å¦åªå‰©ä¸‹test_doc_2
        remaining_doc_ids = [doc.metadata.get('document_id') for doc in remaining_results]
        if len(remaining_results) == 1 and 'test_doc_2' in remaining_doc_ids:
            print("\nâœ… åˆ é™¤åŠŸèƒ½æµ‹è¯•æˆåŠŸï¼åªå‰©ä¸‹é¢„æœŸçš„æ–‡æ¡£")
        else:
            print(f"\nâŒ åˆ é™¤åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼é¢„æœŸåªå‰©ä¸‹test_doc_2ï¼Œå®é™…å‰©ä½™: {remaining_doc_ids}")
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        print("\næ¸…ç†æµ‹è¯•æ•°æ®...")
        vector_store.delete(filter={"knowledge_base_id": {"$eq": 999}})
        print("æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    success = test_pgvector_delete()
    if success:
        print("\nğŸ‰ æ–°ç‰ˆæœ¬PGVectoråˆ é™¤åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nğŸ’¥ æ–°ç‰ˆæœ¬PGVectoråˆ é™¤åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼")
        sys.exit(1)